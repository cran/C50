<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />



<title>Class Probability Calculations</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/rstudio/markdown/inst/resources/prism-xcode.css" data-external="1">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/yihui/knitr/inst/misc/vignette.css" data-external="1">
<script src="https://cdn.jsdelivr.net/combine/npm/@xiee/utils/js/code-lang.min.js,npm/@xiee/utils/js/number-captions.min.js,npm/prismjs@1.29.0/components/prism-core.min.js" data-external="1" defer></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/autoloader/prism-autoloader.min.js" data-external="1" defer></script>

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>











</head>

<body>




<h1 class="title toc-ignore">Class Probability Calculations</h1>



<p>This document describes exactly how the model computes class
probabilities using the data in the terminal nodes. Here is an example
model using the iris data:</p>
<pre class="r"><code>&gt; library(C50)
&gt; mod &lt;- C5.0(Species ~ ., data = iris)
&gt; summary(mod)</code></pre>
<pre><code>
Call:
C5.0.formula(formula = Species ~ ., data = iris)


C5.0 [Release 2.07 GPL Edition]     Wed Apr  2 20:24:11 2025
-------------------------------

Class specified by attribute `outcome&#39;

Read 150 cases (5 attributes) from undefined.data

Decision tree:

Petal.Length &lt;= 1.9: setosa (50)
Petal.Length &gt; 1.9:
:...Petal.Width &gt; 1.7: virginica (46/1)
    Petal.Width &lt;= 1.7:
    :...Petal.Length &lt;= 4.9: versicolor (48/1)
        Petal.Length &gt; 4.9: virginica (6/2)


Evaluation on training data (150 cases):

        Decision Tree   
      ----------------  
      Size      Errors  

         4    4( 2.7%)   &lt;&lt;


       (a)   (b)   (c)    &lt;-classified as
      ----  ----  ----
        50                (a): class setosa
              47     3    (b): class versicolor
               1    49    (c): class virginica


    Attribute usage:

    100.00% Petal.Length
     66.67% Petal.Width


Time: 0.0 secs</code></pre>
<p>Suppose that we are predicting the sample in row 130 with a petal
length of 5.8 and a petal width of 1.6. From this tree, the terminal
node shows <code>virginica (6/2)</code> which means a predicted class of
the virginica species with a probability of 4/6 = 0.66667. However, we
get a different predicted probability:</p>
<pre class="r"><code>&gt; predict(mod, iris[130,], type = &quot;prob&quot;)</code></pre>
<pre><code>        setosa versicolor virginica
130 0.04761905  0.3333333 0.6190476</code></pre>
<p>When we wanted to describe the technical aspects of the <a href="https://www.rulequest.com/see5-info.html">C5.0</a> and <a href="https://www.rulequest.com/cubist-info.html">cubist</a> models, the
main source of information on these models was the raw C source code
from the <a href="https://www.rulequest.com/download.html">RuleQuest
website</a>. For many years, both of these models were proprietary
commercial products and we only recently open-sourced. Our intuition is
that Quinlan quietly evolved these models from the versions described in
the most recent publications to what they are today. For example, it
would not be unreasonable to assume that C5.0 uses <a href="https://en.wikipedia.org/wiki/AdaBoost">AdaBoost</a>. From the
sources, a similar reweighting scheme is used but it does not appear to
be the same.</p>
<p>For classifying new samples, the C sources have</p>
<pre class="c"><code>ClassNo PredictTreeClassify(DataRec Case, Tree DecisionTree){
  ClassNo   c, C;
  double    Prior;
  
  /*  Save total leaf count in ClassSum[0]  */
  ForEach(c, 0, MaxClass) {
    ClassSum[c] = 0;
  }
  
  PredictFindLeaf(Case, DecisionTree, Nil, 1.0);
  
  C = SelectClassGen(DecisionTree-&gt;Leaf, (Boolean)(MCost != Nil), ClassSum);
  
  /*  Set all confidence values in ClassSum  */
  ForEach(c, 1, MaxClass){
    Prior = DecisionTree-&gt;ClassDist[c] / DecisionTree-&gt;Cases;
    ClassSum[c] = (ClassSum[0] * ClassSum[c] + Prior) / (ClassSum[0] + 1);
  }
  Confidence = ClassSum[C];
  
  return C;
}</code></pre>
<p>Here:</p>
<ul>
<li>The predicted probability is the “confidence” value</li>
<li>The prior is the class probabilities from the training set. For the
iris data, this value is 1/3 for each of the classes</li>
<li>The array <code>ClassSum</code> is the probabilities of each class
in the terminal node although <code>ClassSum[0]</code> is the number of
samples in the terminal node (which, if there are missing values, can be
fractional).</li>
</ul>
<p>For sample 130, the virginica values are:</p>
<pre><code>  (ClassSum[0] * ClassSum[c] + Prior) / (ClassSum[0] + 1)
= (          6 *       (4/6) + (1/3)) / (          6 + 1) 
= 0.6190476</code></pre>
<p>Why is it doing this? This will tend to avoid class predictions that
are absolute zero or one.</p>
<p>Basically, it can be viewed to be <em>similar</em> to how Bayesian
methods operate where the simple probability estimates are “shrunken”
towards the prior probabilities. Note that, as the number of samples in
the terminal nodes (<code>ClassSum[0]</code>) becomes large, this
operation has less effect on the final results. Suppose
<code>ClassSum[0] = 10000</code>, then the predicted virginica
probability would be 0.6663337, which is closer to the simple
estimate.</p>
<p>This is very much related to the <a href="https://en.wikipedia.org/wiki/Additive_smoothing">Laplace
Correction</a>. Traditionally, we would add a value of one to the
denominator of the simple estimate and add the number of classes to the
bottom, resulting in <code>(4+1)/(6+3) = 0.5555556</code>. C5.0 is
substituting the prior probabilities and their sum (always one) into
this equation instead.</p>
<p>To be fair, there are well known Bayesian estimates of the sample
proportions under different prior distributions for the two class case.
For example, if there were two classes, the estimate of the class
probability under a uniform prior would be the same as the basic Laplace
correction (using the integers and not the fractions). A more flexible
Bayesian approach is the <a href="https://en.wikipedia.org/wiki/Beta-binomial_distribution">Beta-Binomial
model</a>, which uses a Beta prior instead of the uniform. The downside
here is that two extra parameters need to be estimated (and it only is
defined for two classes)</p>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
